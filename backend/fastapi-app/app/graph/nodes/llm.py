# app/graph/nodes/llm.py
import json
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.8)


async def llm_node(state: dict) -> dict:
    """
    GPT í˜¸ì¶œë¡œ ai_response ìƒì„±
    """
    chat_prompt = state["chat_prompt"]
    response = await llm.ainvoke(chat_prompt)

    try:
        response_json = json.loads(response.content.strip())
        state["ai_response"] = response_json.get("en")
        state["ai_response_kor"] = response_json.get("ko", "")
        state["should_end_call"] = response_json.get("should_end_call", False)
    except json.JSONDecodeError:
        # ì˜ˆì™¸ ì²˜ë¦¬: JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ content ì €ì¥
        # state["ai_response"] = response.content.strip()
        print("ğŸš« JSON Decode Error")
        print("ğŸ’¬ GPT Raw Response:", response.content)
        state["ai_response"] = response.content.strip()
        state["ai_response_kor"] = ""
        state["should_end_call"] = False  # fallback

    return state
